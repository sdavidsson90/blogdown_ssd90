---
title: 'Bike Crash 2: The causes of major injuries'
author: "Sighvatur Davidsson"
date: '2022-02-17'
---

```{r Options, echo= FALSE}
# Supress warnings
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Makes sure error messages get printed in english
Sys.setenv(lang = "en_US")

options(scipen = 999,
        blogdown.knit.on_save = FALSE # Mapping can take some time. Make sure we're not knitting '.Rmd' to '.html' everytime we click save
        )

library(blogdown)
# blogdown::check_site() # Note to self: Try running this if anything goes wrong.
```

### Outline for this post

In this post I will be using machine learning methods to extrude insights that can be gained from the *bike crash*-dataset, that I introduced in my [previous post](https://sdavidsson.netlify.app/post/2022-01-24-introducing-the-bike-crash-data/). In the previous post we looked at the information that has been made available in the dataset, and we concluded that we are working with a dataset that is highly qualitative in nature, hence we must choose appropriate machine learning methods. This is the second part in the series:

1. [Introducing the bike crash-dataset](https://sdavidsson.netlify.app/post/2022-01-24-introducing-the-bike-crash-data/)
2. The causes of major injuries
3. [GIS-tools using the R-command line](https://sdavidsson.netlify.app/post/2022-02-17-bike-crash-3-gis-tools-using-the-r-command-line/)
4. How can these insights make an impact? 

### The business problem
Data analytics is essentially useless if we don't pay close attention to the core business problem we are working on. After all the purpose of machine learning is to gain insights that can be presented to decision makers in order for them to base their decisions on actual data - as opposed to guesswork or general assumptions. I will be discussing this topic further in [Bike Crash 4](google.com). Of course this project  does not have an actual decision maker on the receiving end, but as this is a critical consideration in data analytics, I don't want to leave it out. 

#### Vision zero
In 2015 the North Carolinian transport authorities adopted a new policy for designing the states transportation network. The adoption of this policy legally obliges the authorities to base their traffic improvement measures on the principles of the Vision Zero project. This project strives to reduce the number of traffic related immobilities or deaths to zero. 

The principles are derived from a set of ethical imperatives:
- Citizens lives or mobility can never be traded for other societal advantages
- When citizens lives or mobility are affected preventive measures must be taken

Tingvall & Haworth [(1999)](https://www.monash.edu/muarc/archive/our-publications/papers/visionzero) describe this as a paradigm shift in traffic planning.

#### Thus the question we are trying to answer is: 
#### *Can we identify risk factors that increase the severity of accidents?*

Using this knowledge we would be able to allocate ressources to improving the conditions that have the biggest effect on the seriousness of accidents. 


```{r Workspace setup}
# Load packages
pacman::p_load(tidyverse,                      # essential tools
               sf, tmap,                       # mapping tools
               plotly,                         # interactive plotting
               hrbrthemes,                     # cool plotting theme
               tidymodels, baguette, themis,   # modeling tools
               flextable                       # for creating beautiful tables
)
               
# Set standard plotting theme
theme_set(theme_ipsum_ps())

# Read in datasets
bike_crash <- read_rds("~/R/Projects/bike_crash/processed_data/bike_crash.rds")
```

#### Outcome variable

We will we using the variable *crash_sevr* as our outcome variable.

```{r}
# Before selecting I like to save the unedited dataset to memory
bike_crash0 <- bike_crash

# Initial plotting of our outcome variable
(
  bike_crash %>%
  count(crash_sevr) %>% 
  ggplot(aes(x = crash_sevr, y = n, fill = crash_sevr, text = paste0(crash_sevr,": ", n))) +
  geom_col(fill = "grey60") +
  ggtitle("Severity of crashes (2007 - 2019)") +
  labs(x = "", y = "") +
  theme(axis.text.x=element_text(angle = 30, hjust = 1))
 ) %>% 
  ggplotly(tooltip = c("text")) %>% 
  config(displayModeBar = F)

```

Here we see the frequency distribution of the variable categories. 

As I demonstrated in [part 1](https://sdavidsson.netlify.app/post/2022-01-24-introducing-the-bike-crash-data/) this is very much a qualitative dataset. An obvious choice of machine learning methods is therefore to go with the classification type models. This calls for a recoding of our variable to a dichotomous outcome. 

```{r}
# Outcome variable should be binary with "positive" outcome marked as 1.

# # # # RECODING # # # # # 
# 
#   We're predicting crash severeness, so the encoding will be:
#     1: Serious injury or killed
#     0: Minor injuries
bike_crash <- bike_crash %>% 

  # First we filter out NA's
  filter(!is.na(crash_sevr)) %>% 
  mutate( 
    # Then we use an if_else statement to gather the two most serious categories into 'Major Injuries' and the rest to 'Minor Injuries'
    crash_sevr = if_else(crash_sevr %in% c("Killed", "Suspected Serious Injury"), "Major Injuries", "Minor Injuries"),
   
    # Finally; most classification models require outcome variable to be coded as factor
    crash_sevr = factor(crash_sevr, levels = c("Major Injuries", "Minor Injuries")))

#  Double check recoded variable
contrasts(bike_crash$crash_sevr)

# Plotting of outcome variable after recoding
(
  bike_crash %>%
  count(crash_sevr) %>%
  mutate(crash_sevr = fct_rev(crash_sevr)) %>% 
  ggplot(aes(x = crash_sevr, y = n, fill = crash_sevr, text = paste0(crash_sevr,": ", n))) +
  geom_col(fill = "grey60") +
  labs(x = "", y = "") +
  theme(axis.text.x=element_text(angle = 30, hjust = 1)) 
 ) %>% 
  ggplotly(tooltip = c("text")) %>% 
  config(displayModeBar = F)
```

Here's an updated frequency distribution of our recoded outcome variable. It is very important to notice that we have class imbalance - ie. more than 90 % of the observations are minor injuries. Remember; our objective is to prevent the major injuries by identifying their contributing causes. Left untouched this distribution will almost certainly harm our ability to create a model that can correctly predict the most severe cases. This will be dealt with in the modelling section.

#### Predictor variables
In my modeling for this post I will be focusing on environmental variables - ie. factors in the physical environment that can be improved.

```{r}
# Predictor variables
bike_crash <- bike_crash %>% 
    select(
      # Outcome variable
      crash_sevr, 
      # Environmental variables
      starts_with("rd_"), light_cond, locality, num_lanes, 
      rural_urban, traff_cntrl, speed_limit, workzone, crash_loc, developmen
      ) 

# Option: Omit all NA
# bike_crash <- na.omit(bike_crash)
# Det gør vi ikke, da vi får en bedre model (på alle parametre), hvis vi lader være

colnames(bike_crash)
```

### Modelling

We are now in place to begin our modelling. I will be using testing two different methods against each other; logistic regression and a random forest with bagging. 

```{r}
# I will be modelling using the tidymodels package
# This package was loaded in a previous chunk

# Tidymodels offer control over which model engine to select. 

# Logistic regression
logreg <- logistic_reg() %>% 
  set_engine(engine = "glm") %>%  
  set_mode("classification")

# Bagged random forest (reqires baguette package)
bagged <- bag_tree(min_n = 10) %>%
  set_engine("rpart", times = 25) %>%
  set_mode("classification")

# Define model rest metrics
custom_metrics <- metric_set(accuracy, roc_auc, sensitivity, specificity)
```

We of course have to split the dataset into a train and test set:

```{r}
# Data split for training model
set.seed(512)
bike_split <- initial_split(bike_crash, prop = 0.75, strata = crash_sevr) 
bike_train <- bike_split %>% training()
bike_test <- bike_split %>% testing()

# Cross validation for folds for higher prediction accuracy
set.seed(666)
bike_folds <- vfold_cv(bike_train, v = 2, prop = 0.75,  strata = crash_sevr)

# Define model test metrics
bike_metrics <- metric_set(accuracy, roc_auc, sensitivity, specificity)

# Specify the model formula (outcomes + predictors)
formula <- formula(crash_sevr ~ .)
```

Here are is an initial run of these models:

```{r}
# Define recipe
bike_recipe <- 
  recipe(formula, data = bike_train) %>% 
  step_dummy(all_nominal(), -all_outcomes()) 

# Logistic regression workflow
bc_logreg_wf <- workflow() %>% 
  add_recipe(bike_recipe)  %>% 
  add_model(logreg)

# Random forest workflow
bc_bagged_wf <- workflow() %>% 
  add_recipe(bike_recipe)  %>% 
  add_model(bagged)

# We wan't to collect predictions
ctrl_preds <- control_resamples(save_pred = TRUE)

# Let's fit!
bc_logreg_fit <- fit_resamples(bc_logreg_wf, bike_folds, metrics = custom_metrics,  control = ctrl_preds) 
bc_bagged_fit <- fit_resamples(bc_bagged_wf, bike_folds, metrics = custom_metrics, control = ctrl_preds)

# And evaluate
collect_metrics(bc_logreg_fit) %>% 
  select(-.config, -n) %>% 
  mutate(mean = round(mean, 3),
         std_err = round(std_err, 3)) %>% 
  flextable() %>% add_header_lines("Logistic regression model")

collect_metrics(bc_bagged_fit) %>% 
  select(-.config, -n) %>% 
  mutate(mean = round(mean, 3),
         std_err = round(std_err, 3)) %>% 
  flextable() %>% add_header_lines("Bagged random forest model")
```

In the initial modelling we get a very high accuracy for our models - indicating that it makes correct predictions more than 90 % of the time. This would be a great result if it weren't for the class imbalance we noticed earlier. This means the dataset is biased towards predicting cases that belong to the major class, and thus doesn't necissarily predict the minor class accurately. This is why we need to look at the sensitivity measure. Around 1 % of the time we get correct predictions of the minor class. That's bad. Especially since that is the class we are most interested in predicting correctly. 

We could adjust our model threshold in order to get a higher sensitivity (catch more severe cases), but that would harm our accuracy. 

Because of this we will now try two different algorithms to **resample the dataset**; the rose algorithm; and the downsample-algorithm. These two represent to different ways of resampling. The rose algorithm utilises a k-nearest neighbour algorithm to generate more observations of the minor class in order to balance the outcome class. The downsample algorithm balances the observations by reducing the major class. Therefore they represent two opposite approaches to handling class imbalance.

```{r}
# For one recipe set we apply the rose algorithm
set.seed(444)
bike_recipe_rose <- 
bike_recipe %>% 
  step_rose(crash_sevr) 

# And for the other we test the downsampling algorithm. 
set.seed(555)
bike_recipe_down <- 
bike_recipe %>% 
  step_downsample(crash_sevr) 

# ROSE
bc_logreg_rose_wf <- workflow() %>% 
  add_recipe(bike_recipe_rose)  %>% 
  add_model(logreg)

bc_bagged_rose_wf <- workflow() %>% 
  add_recipe(bike_recipe_rose)  %>% 
  add_model(bagged)

# DOWN
bc_logreg_down_wf <- workflow() %>% 
  add_recipe(bike_recipe_down)  %>% 
  add_model(logreg)

bc_bagged_down_wf <- workflow() %>% 
  add_recipe(bike_recipe_down)  %>% 
  add_model(bagged)

# Let's fit!
bc_logreg_rose_fit <- fit_resamples(bc_logreg_rose_wf, bike_folds, metrics = custom_metrics, control = ctrl_preds)
bc_bagged_rose_fit <- fit_resamples(bc_bagged_rose_wf, bike_folds, metrics = custom_metrics, control = ctrl_preds)

bc_logreg_down_fit <- fit_resamples(bc_logreg_down_wf, bike_folds, metrics = custom_metrics, control = ctrl_preds)
bc_bagged_down_fit <- fit_resamples(bc_bagged_down_wf, bike_folds, metrics = custom_metrics, control = ctrl_preds)


# And evaluate the rose algorithm 
collect_metrics(bc_logreg_rose_fit) %>%
  select(-.config, -n) %>% 
  mutate(mean = round(mean, 3),
         std_err = round(std_err, 3)) %>% 
  flextable() %>% 
  add_header_lines("Logistic regression model (with rose-resampling)")

collect_metrics(bc_bagged_rose_fit) %>% 
  select(-.config, -n) %>% 
  mutate(mean = round(mean, 3),
         std_err = round(std_err, 3)) %>% 
  flextable() %>% 
  add_header_lines("Bagged random forest model (with rose-resampling)")


# And evaluate the downsample algorithm 
collect_metrics(bc_logreg_down_fit) %>% 
  select(-.config, -n) %>% 
  mutate(mean = round(mean, 3),
         std_err = round(std_err, 3)) %>% 
  flextable() %>% 
add_header_lines("Logistic regression model (with downsampling)")

collect_metrics(bc_bagged_down_fit) %>% 
  select(-.config, -n) %>% 
  mutate(mean = round(mean, 3),
         std_err = round(std_err, 3)) %>% 
  flextable() %>% 
  add_header_lines("Bagged random forest model (with downsampling)")
```

We see that the rose algorithm has had a dramatic effect on the logistic regression model, but seems not to have had a great effect on the bagged RF model, which seems to not be working.

It also looks as if the downsampling algorithm has had a positive effect on both models. In both models get a lower accuracy and ROC-AUC (compared to no resampling) but again our sensitivity has increased by a lot, suggesting this model is better able to confine the relationship with our predictors and the outcome variable. 

#### Model selection

But which model do we choose? Since both tables indicate very similar results, neither model is a clear winner.

Considering there is an element of randomness in both the initial train/test-splitting of the dataset and in applying the resampling algorithms, it is very possible that these minute differences could pushed in either direction if we were to set a different seed.

We want to select the model that is best able to predict true positives (ie. has the highest sensitivity) but also  retains an ability to correctly predict true negatives (high specificity). The ROC-AUC is a measure that looks at both the specificity and sensitivity combined and is therefore useful for comparing models. In this case however I would argue that it would be misleading to select the model with the highest AUC. This is because of the class imbalance problem in our outcome variable. Without resampling the ROC-AUC is (somewhat) high for both the logistic regression model and the random forest model. 

For our business problem it is however critical that our model is able to capture the effects leading to major injuries. I therefore suggest we go with the logistic regression model with the rose resampling.

```{r}
# Looks like this one is the winner
collect_metrics(bc_logreg_rose_fit)

# Confusion matrix
(tibble(
  pred = collect_predictions(bc_logreg_rose_fit)$.pred_class,
  truth = bike_train$crash_sevr) %>% 
  conf_mat(truth = truth, estimate = pred) %>% autoplot(type = "heatmap") + 
    ggtitle("Confusion matrix for logistic  \n regression model with rose-resampling") +
    theme(panel.grid.major = element_blank() )) 
```



```{r}
# EVALUATE MODEL ON TEST DATA
bike_last_fit <- last_fit(bc_logreg_rose_wf, bike_split)

# Test_metrics
results <- tibble(
  estimate = bike_last_fit$.predictions[[1]]$.pred_class,
  truth = bike_test$crash_sevr)

accuracy(results, truth, estimate)

bind_rows(
collect_metrics(bike_last_fit),
sensitivity(results, truth, estimate),
specificity(results, truth, estimate))%>%
  select(-.config, -.estimator) %>% 
  mutate(.estimate = round(.estimate, 3)) %>% 
  flextable() %>%  width(width = 2.4) %>% 
  add_header_lines("Logistic regression model (with rose-resampling) - kørt på test-data")

# ROC-curve
roc_data <- tibble(
pred  = bike_last_fit$.predictions[[1]] %>% pull(`.pred_Major Injuries`), 
truth = bike_test$crash_sevr)

roc_curve(roc_data,
          truth = truth, 
          estimate = pred) %>% 
  ggplot(aes(x = 1- specificity, y = sensitivity)) +
  geom_path() +
  coord_equal() +
  geom_abline(lty = 3)
```



```{r, fig.width = 5, fig.height = 15}

# Clean up x_variable names
vars <- bike_last_fit$.workflow[[1]]$fit$fit$preproc$x_var

vars <- 
str_replace_all(vars, pattern = "X|_|\\.|\\s+", " ") %>% 
  str_replace_all(pattern = "\\s+", " " ) %>% 
  str_to_title()

# Visualise variables
(
bike_last_fit %>% 
  pull(.workflow) %>% 
  pluck(1) %>% 
  tidy(exponentiate = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
    mutate(
      term = vars) %>% 
  filter(p.value < 0.05) %>% 
  ggplot(aes(x = estimate, y = fct_reorder(term, estimate))) + 
  geom_point(aes(x = estimate)) +
  geom_errorbar(aes(
    xmin = estimate - std.error,
    xmax = estimate  + std.error )) +
  labs(x = "Odds Ratio + Standard Error", y = "Predictor Variable") 
) %>% 
  ggplotly(tooltip = "estimate", autosize = F, height = 750) %>% 
  config(displayModeBar = F)


# #last_fit_table <-
# bike_last_fit %>%
#   pull(.workflow) %>%
#   pluck(1) %>%
#   tidy(exponentiate = TRUE) %>%
#   filter(term != "(Intercept)",
#          p.value < 0.05) %>%
#   arrange(term) %>%
#   mutate(p.value = round(p.value, 3),
#          statistic = round(statistic, 3),
#          std.error = round(std.error, 3),
#          estimate = round(estimate, 3)) %>%
#   print(n = 100)

```
