---
title: 'Bike Crash 1: Introducing the bike crash-dataset'
author: Sighvatur S. Davidsson
date: '2022-02-04'
---

```{r document, echo= FALSE}
# Supress warnings
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Makes sure error messages get printed in english
Sys.setenv(lang = "en_US")

options(scipen = 999)
```

For my first few blog posts, I will be working with a dataset describing bike crashes in North Carolina. This is very same dataset is also the one that I used for my final thesis studying Data Analytics at Dania Academy. An important criterion for choosing a dataset suitable for a final thesis is to avoid acusations of plagiarizing. This dataset was posted on Kaggle with [an example](https://www.kaggle.com/kerneler/starter-bike-crash-data-2007-2018-0b569cc6-d) of some simple EDA attached - but no machine learning solutions. 

My first four blog posts will therefore be a presentation of some of the learnings I did in that process. Their subject matters are:

1. Introducing the bike crash-dataset
2. Mapping bike crashes with GIS-tools using the command line
3. Identifying environmental factors that affect the number of crashes
4. How can these insights make an impact? 

In this first blog post I will be presenting the dataset. I have already done some [preprocessing](https://github.com/sdavidsson90/bike_crash/blob/main/code/data_preprocessing.R), but here we will be focusing on gaining a general understanding of the dataset.

**Short introduction to the dataset:**

This dataset contains information on crashes between cyclists and motorised vehicles (cars, motorcycles, etc.) taking place in North Carolina in the period 2007 - 2019. The dataset has been made available by the local authorities: North Carolina Division of Motor Vehicles. The information has been sourced from police reports, and manually been put in a tabular form by the University of North Carolina. Overall quality of registrations is therefore assumed to be high. There is however a problem with documentation. We don't have explanations for the variables' meaning but have to rely on their names (which are often abbrevations) and the content of the registrations.

The original dataset can be accesed [here](https://www.pedbikeinfo.org/pbcat_nc/_bicycle.cfm), and officially provided information about the dataset [here](https://www.pedbikeinfo.org/pbcat_nc/_bicycle-about.cfm).

### Let's have a quick look at the dataset

```{r Workspace, echo = TRUE}
# Load packages
pacman::p_load(tidyverse)

# Read in datasets
bike_crash <- read_rds("~/R/Projects/bike_crash/processed_data/bike_crash.rds")

# The quick look
glimpse(bike_crash)

# How many variable types are there?
tibble(
  "var" = names(sapply(bike_crash, class)),
  "type" = sapply(bike_crash, class)) %>%
  count(type)
```

The total number of variables for this dataset is 55. The coordinates of the crash locations do however appear twice - leaving us with 53 distinct variables (where one is an *object id* - ie. specific to the dataset and not the crash registration).

Most of the variables (34) have been read as character variables. In my preprocessing I coded some of the variables to factors, which also should be noted. These are to a varying degree qualitative/non-quantifiable (some examples are *light conditions , crash year, ambulance requirements*). As we do not know the calendar day number we cannot create a precise date-time variable. This is most likely done as an anonymising measure and is to be respected - not reversed. 

Only 10 variables have been read as numeric. Excluding the *object_id* and *date-time* variables, only *the age of driver and cyclist, number of lanes, number of units involved* and *coordinates* have actual numeric meaning.

An important point to note is therefore, that the dataset has a rather qualitative nature. This poses a problem to certain kinds of machine learning methods, but we can adapt our analysis and learning goals to methods that are more fit for this kind if dataset. 

##### How *text heavy* are the variables?

```{r}
bike_crash %>% count(crash_type, sort = T) 
```

This is interesting. We get a detailed description of the accident - but this might also pose a threat to our goal of applying machine learning to this dataset. It's not so much the length of the text, as much the sheer number of categories (65!) that is of concern. This variable will therefore probably not be used in my later modelling. 

Instead take a look at this one:

```{r}
bike_crash %>% count(drvr_veh_typ, sort = T) %>% print(n = 100)
```

We are a bit luckier this time. The level of qualitative learnings we can make from this variable is high, and the number of categories is less (25). This is however still a very high number, but for some of them we see less than 100 registrations. Should we gather these in an *other* category? Maybe. But for now we'll leave them as they are. 


##### How many missing registrations?
Prior to further work on the dataset it is also a good idea to get an impression of missing values across the dataset, as this also might be a cause for concern for some models (eg. the standard random forest model). 

```{r}
# How many missings are there across the dataset?
tibble(
  "variable_name" = colnames(bike_crash),
  "number_of_rows" = nrow(bike_crash),
  "number_of_na" = map_int(bike_crash, ~(sum(is.na(.x)))),
  "pct_missing" = number_of_na/number_of_rows*100) %>% 
  mutate(pct_missing = case_when(pct_missing == Inf ~ 0, TRUE ~ pct_missing)) %>% 
  arrange(-pct_missing) %>% 
  print(n = ncol(bike_crash)) 


# Overall number of missings
str_c(
  "Overall pct missing registrations: ",
    # Number of empty cells
    round(
    (sum(is.na(bike_crash)) /
    # Number of cells
    (ncol(bike_crash) * nrow(bike_crash)) * 100), 2), 
    " % (", sum(is.na(bike_crash)), 
    " out of ", ncol(bike_crash) * nrow(bike_crash),
    ")"
    )

str_c("Number of complete observations: ",
      nrow(na.omit(bike_crash)), 
      ". (Out of ",
      nrow(bike_crash), 
      " original observations total)")

```

We have an overall missing percentage of 3 % which seems quite good. Some models won't work with any missings (NA's) at all, but since we haven't decided (or don't have to make a decision yet) on what type of models we will be working with we don't have to deal with these at the moment. There are several ways to fill in NA's methodically, but for now let's just note that completely eliminating NA's would erase half of the dataset. That prospect doesn't seem good. 

### Initial visualisation of the dataset

##### *Where* do accidents occurr?

First of all it would be interesting to see where accidents occur. Mapping crash locations over such a large area gives us a quite cluttered picture. Adjusting the transparency of the dots can provide us with a clearer picture but other visualisation methods might yield a more correct visualisation. I will show how that can be done in a later post! [upcoming link]()

```{r}
# Load some additional packages
pacman::p_load(sf,             # Essential package for working with geographic data
               tmap            # Mapping tool
               )

# Read in counties geometry
nc_counties <- read_rds("~/R/Projects/bike_crash/processed_data/nc_counties_geom.rds")

# Wehere do bike crashes occur?
st_as_sf(bike_crash, coords = c(x = "x", y = "y"), crs = "NAD83") %>%
  tm_shape() + 
  tm_dots(alpha = 0.1) + 
  tm_shape(nc_counties) + 
  tm_borders(alpha = 0.4, lwd = 1.3) + 
  tm_layout(frame = FALSE,
            main.title = "Crashes in North Carolina 2007-2019", 
            main.title.position = "center",
            main.title.fontfamily = "IBM Plex Sans",
            main.title.size = 1,
            ) 

bike_crash %>% count(rural_urban) %>% 
  mutate(pct = str_c(round(n/sum(n)*100, 0), " %"))

```

The map also suggests that a majority of the crashes take place in urban areas, and less so in rural areas (Rural 28 % vs 72 % urban). This of course not an earth shaking insight, but it is certainly not an irrelevant point for further analysis. Here's a map of the population size by county:

```{r}
# Read in population data
nc_pop <- read_rds("~/R/Projects/bike_crash/processed_data/ncpop_09_19.rds") %>% st_as_sf()

# Create population chloropeth map
tm_shape(nc_pop) +
  tm_polygons("est_2015", title = "Population size (jenks)", style = "jenks") +
  tm_layout(frame = FALSE,
            main.title = "Population size by county (2015)", 
            main.title.position = "center",
            main.title.fontfamily = "IBM Plex Sans",
            main.title.size = .95,
            legend.title.fontfamily = "IBM Plex Sans",
            legend.title.size = 0.85,
            legend.text.fontfamily = "IBM Plex Sans",
            legend.text.size = 0.6,
            legend.width = 8,
            legend.height = 10) 
```

A simple test for correlation confirms this picture. The correlation coefficient for population size and number of accidents pr. county is 0.95. Very high!

```{r}
# We need to specify the geometry once and for all this time
bike_crash <- st_as_sf(bike_crash, coords = c(x = "x", y = "y"), crs = "NAD83")

# Count number of accidents pr. counties
nc_pop <- nc_pop %>% 
  mutate(n_accidents = lengths(st_intersects(., bike_crash)))

# Correlations test 
cor.test(x = nc_pop$est_2015, y =  nc_pop$n_accidents, method = "pearson")

```

### *When* do accidents ocurr?

```{r, warning = FALSE}
# It's better to drop the geometry before making for summarising operations. This speeds up the process significantly!
bike_crash <- st_drop_geometry(bike_crash)

# Set standard theming
theme_set(hrbrthemes::theme_ipsum_ps())

# Plotting crashes over time
cowplot::plot_grid(
  bike_crash %>% count(crash_year) %>% 
    ggplot(aes(x = crash_year, y = n)) + 
    geom_col() + 
    theme(axis.text.x=element_text(angle=90, hjust=1)),
  bike_crash %>% 
    count(crash_month) %>% 
    ggplot(aes(x = crash_month, y = n)) + 
    geom_col() + 
    theme(axis.text.x=element_text(angle=90, hjust=1)),
  bike_crash %>% 
    count(crash_hour) %>% 
    ggplot(aes(x = crash_hour, y = n)) + 
    geom_col()+ 
    theme(axis.text.x=element_text(angle=90, hjust=1)),
  bike_crash %>% 
    count(crash_day) %>% 
    ggplot(aes(x = crash_day, y = n)) + 
    geom_col() + 
    theme(axis.text.x=element_text(angle=90, hjust=1)))

```

Plotting the time dimension also gives us a better understanding of underlying causes behind the accidents. 

- The number of accidents seems to have remained on the same level in the period 2007 - 2019. There are fluctuations but - based on a simple visualisation - I would be hesitant to conclude that we have observed a decrease in the number of accidents over the years. It would also be interesting to compare this to the development of traffic volume in the state (or in specified areas) but unfortunately we don't have acces to this information. 
- There seems to be a seasonality effect at play. The number of accidents begins rising in March and by May we have reached a level which is held until September, when it begins to drop again. This is likely correlated with traffic volumes - ie. more people use their bikes when weather conditions are better. This is however a speculation not backed by data.
- Also the number of accident increases slowly from 5 in the morning until 5 in the evening where we see a steep drop. 
- Finally we see that less accidents take place during the weekends. I will abstain from speculating why. 


# Concluding remarks:
- I have introduced the dataset I will be working with in my first few blogposts.
- The dataset is highly qualitative in nature and is probably best fit for classification-type models and experimenting geographic analysis tools.

Stay tuned!