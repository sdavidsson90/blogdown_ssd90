---
title: "Introducing the bike crash dataset"
author: "Sighvatur S. Davidsson"
date: '2022-02-04'
slug: []
categories: []
tags: []
---

```{r , echo= FALSE}
# Supress warnings
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Makes sure error messages get printed in english
Sys.setenv(lang = "en_US")

options(scipen = 999)

```

For my first few blog posts, I plan on using a dataset describing *bike crashes* in North Carolina. I also used this very dataset in my final thesis studying Data Analytics at Dania Academy. An important criterion for choosing a dataset suitable for a final thesis is to avoid being acused of plagiarizing. The dataset has been posted on Kaggle, but no machine learning solutions have been posted. 

I have already done some [preprocessing](https://github.com/sdavidsson90/bike_crash/blob/main/code/data_preprocessing.R), and now I want to present some of the challanges and oppurtunities that present themselves in working with this dataset. 

**Short introduction to the dataset:**

This dataset contains information on crashes between cyclists and motorised vehicles (cars, motorcycles, etc.) taking place in North Carolina in the period 2007 - 2019. The dataset has been made available by the local authorities: North Carolina Division of Motor Vehicles. The information has been sourced from police reports, and manually been put in a tabular form by the University of North Carolina. Overall quality of registrations is therefore assumed to be high. There is however a problem with documentation. We don't have explanations for the variables' meaning but have to rely on their names (which are often abbrevations) and the content of the registrations.

The original dataset can be accesed [here](https://www.pedbikeinfo.org/pbcat_nc/_bicycle.cfm), and officially provided information about the dataset [here](https://www.pedbikeinfo.org/pbcat_nc/_bicycle-about.cfm).

### Let's have a quick look at the dataset
```{r Workspace, echo = TRUE}
# Load packages
pacman::p_load(tidyverse)

# Read in datasets
bike_crash <- read_rds("~/R/Projects/bike_crash/processed_data/bike_crash.rds")

# Get number of datatypes
skimr::skim(bike_crash)
```

If we think of the coordinate variable (representing crash location) as one variable - we have a total number of 52 variables. Most of the variables (34) have been read as character variables, and some have been coded as factors in the preprocessing I did prior to this post. Excluding the *coordinates* and *crash_id-variable*, only the age of driver and cyclist, number of lanes and the number of units involved have been read as actual numeric variables. 

An important point to note is therefore, that the dataset has a qualitative nature and is often quite text heavy (as will be pointed out shortly). This will pose a problem to certain machine learning methods, but we will have to adapt our analysis to methods fit for this kind of dataset - classification methods and geographic analysis tools might be a good place to start. 

##### How many missing registrations?
Also we have a number of missing values across the dataset. This is also a problem for some models (eg. the standard random forest model), but we can work around this. 

```{r}
# Number of empty cells
(sum(is.na(bike_crash)) /
# Number of cells
(ncol(bike_crash) * nrow(bike_crash)) * 100)

# How many missings are there across the dataset?
tibble(
  "variable_name" = colnames(bike_crash),
  "number_of_rows" = nrow(bike_crash),
  "number_of_na" = map_int(bike_crash, ~(sum(is.na(.x)))),
  "pct_missing" = number_of_na/number_of_rows*100) %>% 
  mutate(pct_missing = case_when(pct_missing == Inf ~ 0, TRUE ~ pct_missing)) %>% 
  print(n = ncol(bike_crash))
```

##### How text heavy are the variables?

```{r}
bike_crash %>% count(crash_type, sort = T) 
```

Not only is the length of the text, but also the sheer number of categories (65!). This variable will therefore problably be used in my modelling (unless we look specifically at a certain situation). But take a look at this one:

```{r}
bike_crash %>% count(drvr_veh_typ, sort = T) %>% print(n = 100)
```

We are a bit luckier this time. We have 25 categories but for 19 of them we have less than 100 registrations. Are we to gather these in an other category? Maybe. But for now we'll leave them as they are. 

### Initial visualisation of the dataset

##### *Where* do accidents occurr?

First of all it would be interesting to see where accidents occur. Mapping crash locations over such a large area gives us a quite cluttered picture. Adjusting the transparency of the dots can provide us with a clearer picture but other visualisation methods might yield a more correct visualisation. I will show how that can be done in a later post! [upcoming link] 

```{r}
# Load some additional packages
pacman::p_load(sf,             # Essential package for working with geographic data
               tmap            # Mapping tool
               )

# Read in counties geometry
nc_counties <- read_rds("~/R/Projects/bike_crash/processed_data/nc_counties_geom.rds")

# Wehere do bike crashes occur?
st_as_sf(bike_crash, coords = c(x = "x", y = "y"), crs = "NAD83") %>%
  tm_shape() + 
  tm_dots(alpha = 0.1) + 
  tm_shape(nc_counties) + 
  tm_borders(alpha = 0.4, lwd = 1.3) + 
  tm_layout(frame = TRUE,
            main.title = "Crashes in North Carolina 2007-2019", 
            main.title.position = "center",
            main.title.fontfamily = "IBM Plex Sans",
            main.title.size = 1,
            ) 
```


The map also suggests that a majority of the crashes take place in urban areas, and less so in rural areas. This of course not an earth shaking insight, but it is certainly not an irrelevant point for further analysis. 

Here's a map of the population size by county:

```{r}
# Read in population data
nc_pop <- read_rds("~/R/Projects/bike_crash/processed_data/ncpop_09_19.rds") %>% st_as_sf()

# Create population chloropeth map
tm_shape(nc_pop) +
  tm_polygons("est_2015", title = "Population size (jenks)", style = "jenks") + 
  tm_layout(main.title = "Population size by county (2015)", 
            main.title.position = "center",
            main.title.fontfamily = "IBM Plex Sans",
            main.title.size = .95,
            frame = TRUE,
            legend.title.fontfamily = "IBM Plex Sans",
            legend.title.size = 0.85,
            legend.text.fontfamily = "IBM Plex Sans",
            legend.text.size = 0.6,
            legend.width = 8,
            legend.height = 10) 
```

And a simple test for correlation confirms this picture. The correlation coefficient for population size and number of accidents pr. county is 0.95. Very high!

```{r}
# We need to specify the geometry once and for all this time
bike_crash <- st_as_sf(bike_crash, coords = c(x = "x", y = "y"), crs = "NAD83")

# Count number of accidents pr. counties
nc_pop <- nc_pop %>% 
  mutate(n_accidents = lengths(st_intersects(., bike_crash)))

# Correlations test 
cor.test(x = nc_pop$est_2015, y =  nc_pop$n_accidents, method = "pearson")

```
### *When* do accidents ocurr?

```{r, warning = FALSE}
# It's better to drop the geometry before making for summarising operations. This speeds up the process significantly!
bike_crash <- st_drop_geometry(bike_crash)

# Set standard theming
theme_set(hrbrthemes::theme_ipsum_ps())

# Plotting crashes over time
cowplot::plot_grid(
  bike_crash %>% count(crash_year) %>% 
    ggplot(aes(x = crash_year, y = n)) + 
    geom_col() + 
    theme(axis.text.x=element_text(angle=90, hjust=1)),
  bike_crash %>% 
    count(crash_month) %>% 
    ggplot(aes(x = crash_month, y = n)) + 
    geom_col() + 
    theme(axis.text.x=element_text(angle=90, hjust=1)),
  bike_crash %>% 
    count(crash_hour) %>% 
    ggplot(aes(x = crash_hour, y = n)) + 
    geom_col()+ 
    theme(axis.text.x=element_text(angle=90, hjust=1)),
  bike_crash %>% 
    count(crash_day) %>% 
    ggplot(aes(x = crash_day, y = n)) + 
    geom_col() + 
    theme(axis.text.x=element_text(angle=90, hjust=1)))
```

Plotting the time dimension also gives us a better understanding of underlying causes behind the accidents. 

- Unfortunately the number of accidents seems to have remained on the same level in the period 2007 - 2019. There are fluctuations but - based on a simple visualisation - I would be hesitant to conclude that we have observed a decrease in the number of accidents over the years. It would also be interesting to compare this to the development of traffic volume in the state (or in specified areas) but unfortunately we don't have acces to this information. 
- There seems to be a seasonality effect at play. The number of accidents begins rising in March and by May we have reached a level which is held until September, when it begins to drop again. This is likely correlated with traffic volumes - ie. more people use their bikes when weather conditions are better. This is however a speculation not backed by data.
- Also the number of accident increases slowly from 5 in the morning until 5 in the evening where we see a steep drop. 
- Finally we see that less accidents take place during the weekends. I will abstain from speculating why. 

# Concluding remarks:
- I have introduced the dataset I will be working with in my first few blogposts.
- The dataset is highly qualitative in nature and is probably best fit for classification-type models and experimenting geographic analysis tools.

Stay tuned!


